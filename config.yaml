server:
  host: "0.0.0.0"
  port: 8081

database:
  url: "" # Set DATABASE_URL in .env

# AI Providers â€” uncomment and configure the ones you need.
# Each provider name becomes the prefix for model IDs (e.g., "openai/gpt-4o").
providers:
  # OpenAI (also works for any OpenAI-compatible API like Ollama, LM Studio)
  # openai:
  #   type: openai
  #   url: "https://api.openai.com/v1"
  #   api_key: "sk-..."
  #
  # Ollama (local LLMs via OpenAI-compatible API)
  ollama:
    type: openai
    url: "http://localhost:11434/v1"
    api_key: ""
  #
  # Anthropic (Claude)
  # anthropic:
  #   type: anthropic
  #   url: "https://api.anthropic.com"
  #   api_key: "sk-ant-..."
  #
  # Claude Code (uses claude CLI subscription, no API key needed)
  claude:
    type: claude-code
  #
  # Google Gemini (text)
  gemini:
    type: gemini
    url: "https://generativelanguage.googleapis.com"
    api_key: "" # Set GEMINI_API_KEY in .env
  #
  # Google Gemini Image Generation (Nano Banana)
  nano-banana:
    type: gemini-image
    api_key: "" # Set NANO_BANANA_API_KEY in .env
  #
  # Z-IMAGE local inference server (run: python scripts/zimage_server.py)
  zimage:
    type: zimage
    url: "http://localhost:8090"
  #
  # OpenAI TTS (text-to-speech)
  # openai-tts:
  #   type: openai-tts
  #   api_key: "" # Set OPENAI_TTS_API_KEY in .env

mcp_servers: {}
