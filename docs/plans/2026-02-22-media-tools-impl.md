# Media Tools Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Add TTS (model), video_merge and remotion_render (tools) to enable YouTube automation workflows.

**Architecture:** TTS follows the existing image model pattern — `adkmodel.LLM` implementation returning audio `InlineData`, with `llm_builder` saving audio to disk and storing the file path in session state. Video merge and Remotion render are `tools.Tool` implementations registered in the tool registry. SRT is generated by the script agent directly, no new infrastructure needed.

**Tech Stack:** Go 1.23, `adkmodel.LLM` interface, OpenAI TTS API, FFmpeg (external), Node.js + `@remotion/renderer` (external)

**Design doc:** [docs/plans/2026-02-22-media-tools-design.md](./2026-02-22-media-tools-design.md)

---

### Task 1: Add OutputDir to BuildDeps and thread through service layer

Audio files need a place to be saved. `BuildDeps` (used by `llm_builder`) needs an `OutputDir` field. Thread it from `main.go` → `WorkflowService` → `BuildDeps`.

**Files:**
- Modify: `internal/agents/registry.go:28-31`
- Modify: `internal/services/workflow.go:39-53`
- Modify: `cmd/upal/main.go` (outputs dir creation + pass to service)

**Step 1: Add OutputDir to BuildDeps**

In `internal/agents/registry.go`, update the struct:
```go
type BuildDeps struct {
	LLMs      map[string]adkmodel.LLM
	ToolReg   *tools.Registry
	OutputDir string // directory for saving media outputs (audio, video)
}
```

**Step 2: Update WorkflowService to accept outputDir**

In `internal/services/workflow.go`, change `NewWorkflowService`:
```go
func NewWorkflowService(
	repo repository.WorkflowRepository,
	llms map[string]adkmodel.LLM,
	sessionService session.Service,
	toolReg *tools.Registry,
	nodeRegistry *agents.NodeRegistry,
	outputDir string,
) *WorkflowService {
	return &WorkflowService{
		repo:           repo,
		llms:           llms,
		sessionService: sessionService,
		toolReg:        toolReg,
		nodeRegistry:   nodeRegistry,
		buildDeps:      agents.BuildDeps{LLMs: llms, ToolReg: toolReg, OutputDir: outputDir},
	}
}
```

**Step 3: Create outputs dir and pass to service in main.go**

In `cmd/upal/main.go`, after `dataDir` creation:
```go
outputDir := "outputs"
if err := os.MkdirAll(outputDir, 0755); err != nil {
    slog.Error("failed to create outputs directory", "err", err)
    os.Exit(1)
}
```

Update `NewWorkflowService` call:
```go
workflowSvc := services.NewWorkflowService(repo, llms, sessionService, toolReg, nodeReg, outputDir)
```

**Step 4: Verify it compiles**

```bash
go build ./...
```
Expected: no errors. Fix any callers of `NewWorkflowService` in tests.

**Step 5: Fix workflow_test.go if needed**

`internal/services/workflow_test.go` likely calls `NewWorkflowService` — add `""` as the last arg (empty outputDir is fine for tests).

**Step 6: Run tests**

```bash
go test ./internal/services/... -v -race
```
Expected: PASS

**Step 7: Commit**
```bash
git add internal/agents/registry.go internal/services/workflow.go cmd/upal/main.go internal/services/workflow_test.go
git commit -m "feat: add OutputDir to BuildDeps and thread through WorkflowService"
```

---

### Task 2: Add audio file saving to llmutil

When `llm_builder` gets a response with audio `InlineData`, it should save the binary to disk and return the file path string instead of a data URI.

**Files:**
- Modify: `internal/llmutil/response.go`
- Create: `internal/llmutil/response_test.go` (partial — audio extraction)

**Step 1: Write failing test**

Create `internal/llmutil/response_test.go` (add to existing or create new file):
```go
package llmutil_test

import (
    "os"
    "path/filepath"
    "strings"
    "testing"

    "github.com/soochol/upal/internal/llmutil"
    adkmodel "google.golang.org/adk/model"
    "google.golang.org/genai"
)

func TestExtractContentWithAudio(t *testing.T) {
    dir := t.TempDir()
    audioData := []byte("fake-mp3-data")

    resp := &adkmodel.LLMResponse{
        Content: &genai.Content{
            Parts: []*genai.Part{
                {InlineData: &genai.Blob{Data: audioData, MIMEType: "audio/mpeg"}},
            },
        },
    }

    result := llmutil.ExtractContentSavingAudio(resp, dir)

    if !strings.HasSuffix(result, ".mp3") {
        t.Errorf("expected .mp3 path, got %q", result)
    }
    if _, err := os.Stat(result); err != nil {
        t.Errorf("file not found at path %q: %v", result, err)
    }
    data, _ := os.ReadFile(result)
    if string(data) != string(audioData) {
        t.Errorf("file content mismatch")
    }
    _ = filepath.Dir(result) // just use it
}

func TestExtractContentSavingAudio_TextPassthrough(t *testing.T) {
    resp := &adkmodel.LLMResponse{
        Content: &genai.Content{
            Parts: []*genai.Part{genai.NewPartFromText("hello world")},
        },
    }
    result := llmutil.ExtractContentSavingAudio(resp, t.TempDir())
    if result != "hello world" {
        t.Errorf("expected 'hello world', got %q", result)
    }
}

func TestExtractContentSavingAudio_ImageStillDataURI(t *testing.T) {
    imgData := []byte{0x89, 0x50, 0x4E, 0x47} // PNG header
    resp := &adkmodel.LLMResponse{
        Content: &genai.Content{
            Parts: []*genai.Part{
                {InlineData: &genai.Blob{Data: imgData, MIMEType: "image/png"}},
            },
        },
    }
    result := llmutil.ExtractContentSavingAudio(resp, t.TempDir())
    if !strings.HasPrefix(result, "data:image/png;base64,") {
        t.Errorf("expected image data URI, got %q", result)
    }
}
```

**Step 2: Run test to confirm it fails**

```bash
go test ./internal/llmutil/... -run TestExtractContent -v
```
Expected: FAIL — `ExtractContentSavingAudio undefined`

**Step 3: Implement ExtractContentSavingAudio**

Add to `internal/llmutil/response.go`:
```go
import (
    "encoding/base64"
    "fmt"
    "os"
    "path/filepath"
    "strings"

    "github.com/google/uuid"
    adkmodel "google.golang.org/adk/model"
)

// audioMIMEToExt maps audio MIME types to file extensions.
var audioMIMEToExt = map[string]string{
    "audio/mpeg": ".mp3",
    "audio/mp3":  ".mp3",
    "audio/wav":  ".wav",
    "audio/ogg":  ".ogg",
    "audio/webm": ".webm",
}

// ExtractContentSavingAudio is like ExtractContent but saves audio InlineData
// to outputDir instead of encoding as a data URI. Returns the file path for
// audio parts. Image InlineData is still returned as data URIs.
// If outputDir is empty, audio falls back to data URI behaviour.
func ExtractContentSavingAudio(resp *adkmodel.LLMResponse, outputDir string) string {
    if resp == nil || resp.Content == nil {
        return ""
    }
    var parts []string
    for _, p := range resp.Content.Parts {
        if p.Text != "" {
            parts = append(parts, p.Text)
            continue
        }
        if p.InlineData == nil || len(p.InlineData.Data) == 0 {
            continue
        }
        mime := p.InlineData.MIMEType
        ext, isAudio := audioMIMEToExt[mime]
        if isAudio && outputDir != "" {
            path, err := saveToFile(p.InlineData.Data, outputDir, ext)
            if err != nil {
                // Fall back to data URI on save failure.
                parts = append(parts, fmt.Sprintf("data:%s;base64,%s", mime,
                    base64.StdEncoding.EncodeToString(p.InlineData.Data)))
            } else {
                parts = append(parts, path)
            }
            continue
        }
        // Image or unknown: data URI.
        parts = append(parts, fmt.Sprintf("data:%s;base64,%s", mime,
            base64.StdEncoding.EncodeToString(p.InlineData.Data)))
    }
    return strings.Join(parts, "\n")
}

// saveToFile writes data to a uniquely named file in dir and returns the path.
func saveToFile(data []byte, dir, ext string) (string, error) {
    if err := os.MkdirAll(dir, 0755); err != nil {
        return "", fmt.Errorf("create output dir: %w", err)
    }
    name := filepath.Join(dir, uuid.New().String()+ext)
    if err := os.WriteFile(name, data, 0644); err != nil {
        return "", fmt.Errorf("write file: %w", err)
    }
    return name, nil
}
```

Note: uses `github.com/google/uuid`. Check go.mod — if absent, add:
```bash
go get github.com/google/uuid
```

**Step 4: Run tests**

```bash
go test ./internal/llmutil/... -v -race
```
Expected: PASS

**Step 5: Commit**
```bash
git add internal/llmutil/response.go internal/llmutil/response_test.go
git commit -m "feat(llmutil): add ExtractContentSavingAudio — saves audio InlineData to disk"
```

---

### Task 3: Use audio-aware extraction in llm_builder

`llm_builder.go` currently calls `llmutil.ExtractContent`. Switch to `ExtractContentSavingAudio` using the `OutputDir` from `BuildDeps`.

**Files:**
- Modify: `internal/agents/llm_builder.go`

**Step 1: Update the final result extraction in llm_builder**

In `llm_builder.go`, the `Build` function captures `deps BuildDeps`. Capture `outputDir` at build time:

```go
func (b *LLMNodeBuilder) Build(nd *upal.NodeDefinition, deps BuildDeps) (agent.Agent, error) {
    // ... existing code ...
    outputDir := deps.OutputDir  // add this line near the top of Build
    // ...
```

Find the line (around line 186):
```go
result := strings.TrimSpace(llmutil.ExtractContent(resp))
```

Replace with:
```go
result := strings.TrimSpace(llmutil.ExtractContentSavingAudio(resp, outputDir))
```

**Step 2: Run tests**

```bash
go test ./internal/agents/... -v -race
```
Expected: PASS (no behaviour change for text/image models)

**Step 3: Commit**
```bash
git add internal/agents/llm_builder.go
git commit -m "feat(agents): use ExtractContentSavingAudio in llm_builder for audio output support"
```

---

### Task 4: Implement OpenAI TTS model

Create `TTSModel` implementing `adkmodel.LLM`. Maps `system_prompt` → instructions, `prompt` text → audio binary returned as `InlineData`.

**Files:**
- Create: `internal/model/tts.go`
- Create: `internal/model/tts_test.go`

**Step 1: Write failing test**

Create `internal/model/tts_test.go`:
```go
package model

import (
    "context"
    "encoding/json"
    "io"
    "net/http"
    "net/http/httptest"
    "testing"

    adkmodel "google.golang.org/adk/model"
    "google.golang.org/genai"
)

func TestOpenAITTSGenerateContent(t *testing.T) {
    fakeAudio := []byte("fake-mp3-audio-data")

    server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        if r.URL.Path != "/audio/speech" {
            t.Errorf("unexpected path: %s", r.URL.Path)
        }
        if r.Method != http.MethodPost {
            t.Errorf("unexpected method: %s", r.Method)
        }

        var body map[string]any
        _ = json.NewDecoder(r.Body).Decode(&body)

        if body["input"] != "Hello world" {
            t.Errorf("unexpected input: %v", body["input"])
        }
        if body["model"] != "tts-1" {
            t.Errorf("unexpected model: %v", body["model"])
        }
        if body["instructions"] != "밝게 말하세요" {
            t.Errorf("unexpected instructions: %v", body["instructions"])
        }

        w.Header().Set("Content-Type", "audio/mpeg")
        w.WriteHeader(http.StatusOK)
        w.Write(fakeAudio)
    }))
    defer server.Close()

    tts := NewOpenAITTSModel("test-key", server.URL)

    req := &adkmodel.LLMRequest{
        Model: "tts-1",
        Config: &genai.GenerateContentConfig{
            SystemInstruction: genai.NewContentFromText("밝게 말하세요", genai.RoleUser),
        },
        Contents: []*genai.Content{
            genai.NewContentFromText("Hello world", genai.RoleUser),
        },
    }

    var resp *adkmodel.LLMResponse
    for r, err := range tts.GenerateContent(context.Background(), req, false) {
        if err != nil {
            t.Fatalf("unexpected error: %v", err)
        }
        resp = r
    }

    if resp == nil || resp.Content == nil {
        t.Fatal("nil response")
    }
    if len(resp.Content.Parts) != 1 {
        t.Fatalf("expected 1 part, got %d", len(resp.Content.Parts))
    }
    p := resp.Content.Parts[0]
    if p.InlineData == nil {
        t.Fatal("expected InlineData, got nil")
    }
    if p.InlineData.MIMEType != "audio/mpeg" {
        t.Errorf("unexpected MIME type: %s", p.InlineData.MIMEType)
    }
    if string(p.InlineData.Data) != string(fakeAudio) {
        t.Errorf("audio data mismatch")
    }
}

func TestOpenAITTSName(t *testing.T) {
    tts := NewOpenAITTSModel("key", "http://localhost")
    if tts.Name() != "openai-tts" {
        t.Errorf("unexpected name: %s", tts.Name())
    }
}
```

**Step 2: Run test to confirm it fails**

```bash
go test ./internal/model/... -run TestOpenAITTS -v
```
Expected: FAIL — `NewOpenAITTSModel undefined`

**Step 3: Implement OpenAI TTS model**

Create `internal/model/tts.go`:
```go
package model

import (
    "bytes"
    "context"
    "encoding/json"
    "fmt"
    "io"
    "iter"
    "net/http"
    "strings"

    adkmodel "google.golang.org/adk/model"
    "google.golang.org/genai"
)

const defaultOpenAITTSBaseURL = "https://api.openai.com/v1"

// OpenAITTSModel implements adkmodel.LLM for OpenAI's TTS API.
// system_prompt → speaking instructions, prompt → text to speak.
// Returns audio binary as InlineData (audio/mpeg).
type OpenAITTSModel struct {
    apiKey  string
    baseURL string
    client  *http.Client
}

// NewOpenAITTSModel creates an OpenAI TTS adapter.
// baseURL defaults to https://api.openai.com/v1 if empty.
func NewOpenAITTSModel(apiKey, baseURL string) *OpenAITTSModel {
    if baseURL == "" {
        baseURL = defaultOpenAITTSBaseURL
    }
    return &OpenAITTSModel{
        apiKey:  apiKey,
        baseURL: strings.TrimRight(baseURL, "/"),
        client:  &http.Client{},
    }
}

func (t *OpenAITTSModel) Name() string { return "openai-tts" }

func (t *OpenAITTSModel) GenerateContent(ctx context.Context, req *adkmodel.LLMRequest, stream bool) iter.Seq2[*adkmodel.LLMResponse, error] {
    return func(yield func(*adkmodel.LLMResponse, error) bool) {
        resp, err := t.generate(ctx, req)
        yield(resp, err)
    }
}

type ttsRequest struct {
    Model        string `json:"model"`
    Input        string `json:"input"`
    Voice        string `json:"voice,omitempty"`
    Instructions string `json:"instructions,omitempty"`
}

func (t *OpenAITTSModel) generate(ctx context.Context, req *adkmodel.LLMRequest) (*adkmodel.LLMResponse, error) {
    // Extract text to speak from contents.
    var input string
    for _, c := range req.Contents {
        for _, p := range c.Parts {
            if p.Text != "" {
                input += p.Text
            }
        }
    }
    if input == "" {
        return nil, fmt.Errorf("openai-tts: no text to speak")
    }

    // Extract speaking instructions from system prompt.
    var instructions string
    if req.Config != nil && req.Config.SystemInstruction != nil {
        for _, p := range req.Config.SystemInstruction.Parts {
            if p.Text != "" {
                instructions = p.Text
                break
            }
        }
    }

    modelName := req.Model
    if modelName == "" {
        modelName = "tts-1"
    }

    body := ttsRequest{
        Model:        modelName,
        Input:        input,
        Voice:        "alloy", // default voice; can be overridden via model name suffix
        Instructions: instructions,
    }

    encoded, err := json.Marshal(body)
    if err != nil {
        return nil, fmt.Errorf("openai-tts: marshal request: %w", err)
    }

    httpReq, err := http.NewRequestWithContext(ctx, http.MethodPost,
        t.baseURL+"/audio/speech", bytes.NewReader(encoded))
    if err != nil {
        return nil, fmt.Errorf("openai-tts: create request: %w", err)
    }
    httpReq.Header.Set("Content-Type", "application/json")
    httpReq.Header.Set("Authorization", "Bearer "+t.apiKey)

    httpResp, err := t.client.Do(httpReq)
    if err != nil {
        return nil, fmt.Errorf("openai-tts: HTTP request failed: %w", err)
    }
    defer httpResp.Body.Close()

    audioData, err := io.ReadAll(httpResp.Body)
    if err != nil {
        return nil, fmt.Errorf("openai-tts: read response: %w", err)
    }

    if httpResp.StatusCode != http.StatusOK {
        return nil, fmt.Errorf("openai-tts: server returned %d: %s", httpResp.StatusCode, string(audioData))
    }

    mimeType := httpResp.Header.Get("Content-Type")
    if mimeType == "" {
        mimeType = "audio/mpeg"
    }
    // Normalize: "audio/mpeg" covers mp3
    if strings.HasPrefix(mimeType, "audio/") {
        mimeType = strings.SplitN(mimeType, ";", 2)[0]
    }

    return &adkmodel.LLMResponse{
        Content: &genai.Content{
            Role: genai.RoleModel,
            Parts: []*genai.Part{
                {InlineData: &genai.Blob{Data: audioData, MIMEType: mimeType}},
            },
        },
        TurnComplete: true,
        FinishReason: genai.FinishReasonStop,
    }, nil
}
```

**Step 4: Run tests**

```bash
go test ./internal/model/... -run TestOpenAITTS -v -race
```
Expected: PASS

**Step 5: Commit**
```bash
git add internal/model/tts.go internal/model/tts_test.go
git commit -m "feat(model): add OpenAITTSModel — TTS as adkmodel.LLM, returns audio InlineData"
```

---

### Task 5: Register TTS in API models

Add `ModelCategoryTTS` so the frontend and generator know about TTS models.

**Files:**
- Modify: `internal/api/models.go`

**Step 1: Add category constant and mappings**

In `internal/api/models.go`:

```go
// Add to const block:
ModelCategoryTTS ModelCategory = "tts"

// Add to modelCategoryByType:
"openai-tts": ModelCategoryTTS,

// Add to categoryOptions:
ModelCategoryTTS: {
    {Key: "voice", Label: "Voice", Type: "select", Default: "alloy", Choices: []OptionChoice{
        {Label: "Alloy", Value: "alloy"},
        {Label: "Echo", Value: "echo"},
        {Label: "Fable", Value: "fable"},
        {Label: "Onyx", Value: "onyx"},
        {Label: "Nova", Value: "nova"},
        {Label: "Shimmer", Value: "shimmer"},
    }},
},

// Add to knownModels:
"openai-tts": {
    {"tts-1", ModelTierLow, "fast speech synthesis, standard quality"},
    {"tts-1-hd", ModelTierMid, "high-definition speech synthesis"},
},
```

**Step 2: Compile check**

```bash
go build ./internal/api/...
```
Expected: no errors

**Step 3: Commit**
```bash
git add internal/api/models.go
git commit -m "feat(api): add ModelCategoryTTS with OpenAI TTS model entries"
```

---

### Task 6: Register TTS provider in main.go

Wire up the TTS model so it is instantiated from config.

**Files:**
- Modify: `cmd/upal/main.go`

**Step 1: Add case to provider switch**

In the `switch pc.Type` block in `serve()`:
```go
case "openai-tts":
    llms[name] = upalmodel.NewOpenAITTSModel(pc.APIKey, pc.URL)
```

**Step 2: Add example to config.yaml comment (optional — do not modify actual config.yaml)**

Document in the plan: users add to `config.yaml`:
```yaml
providers:
  openai-tts:
    type: openai-tts
    api_key: sk-...
```

**Step 3: Compile check**

```bash
go build ./cmd/upal/...
```
Expected: no errors

**Step 4: Commit**
```bash
git add cmd/upal/main.go
git commit -m "feat(main): register openai-tts provider type"
```

---

### Task 7: Update generate.go for TTS model category

The generator's `buildModelPrompt` should guide the LLM to use TTS models for speech synthesis nodes.

**Files:**
- Modify: `internal/generate/generate.go:282-314`

**Step 1: Add TTS section to buildModelPrompt**

After the `image` section in `buildModelPrompt`:
```go
if tts := groups["tts"]; len(tts) > 0 {
    b.WriteString("\nTTS (text-to-speech) models — use ONLY when the task requires converting text to spoken audio:\n")
    for _, m := range tts {
        fmt.Fprintf(&b, "- %q — %s\n", m.ID, m.Hint)
    }
}
```

Also update the `MODEL SELECTION RULES` string to mention TTS:
```
2. Choose the model category that matches the node's PURPOSE: text models for reasoning/text tasks, image models for image generation tasks, tts models for speech synthesis tasks.
```

**Step 2: Update generate_test.go if needed**

Check `internal/generate/generate_test.go` — if it asserts on the prompt string containing model categories, update accordingly.

**Step 3: Run tests**

```bash
go test ./internal/generate/... -v -race
```
Expected: PASS

**Step 4: Commit**
```bash
git add internal/generate/generate.go
git commit -m "feat(generate): add TTS model category guidance to workflow generator"
```

---

### Task 8: Implement video_merge tool

FFmpeg-based tool to combine video and audio files.

**Files:**
- Create: `internal/tools/video_merge.go`
- Create: `internal/tools/video_merge_test.go`

**Step 1: Write failing test (input validation only — FFmpeg not required)**

Create `internal/tools/video_merge_test.go`:
```go
package tools

import (
    "context"
    "testing"
)

func TestVideoMergeTool_Name(t *testing.T) {
    tool := &VideoMergeTool{}
    if tool.Name() != "video_merge" {
        t.Errorf("expected 'video_merge', got %q", tool.Name())
    }
}

func TestVideoMergeTool_InputValidation(t *testing.T) {
    tool := &VideoMergeTool{}

    t.Run("missing inputs", func(t *testing.T) {
        _, err := tool.Execute(context.Background(), map[string]any{
            "mode": "mux_audio",
        })
        if err == nil {
            t.Error("expected error for missing inputs")
        }
    })

    t.Run("invalid input type", func(t *testing.T) {
        _, err := tool.Execute(context.Background(), map[string]any{
            "inputs": "not-an-array",
        })
        if err == nil {
            t.Error("expected error for invalid inputs type")
        }
    })

    t.Run("empty inputs array", func(t *testing.T) {
        _, err := tool.Execute(context.Background(), map[string]any{
            "inputs": []any{},
        })
        if err == nil {
            t.Error("expected error for empty inputs")
        }
    })
}

func TestVideoMergeTool_Schema(t *testing.T) {
    tool := &VideoMergeTool{}
    schema := tool.InputSchema()
    if schema["type"] != "object" {
        t.Errorf("expected object schema")
    }
    props, ok := schema["properties"].(map[string]any)
    if !ok {
        t.Fatal("missing properties")
    }
    if _, ok := props["inputs"]; !ok {
        t.Error("missing 'inputs' property")
    }
    if _, ok := props["mode"]; !ok {
        t.Error("missing 'mode' property")
    }
}
```

**Step 2: Run test to confirm it fails**

```bash
go test ./internal/tools/... -run TestVideoMerge -v
```
Expected: FAIL — `VideoMergeTool undefined`

**Step 3: Implement VideoMergeTool**

Create `internal/tools/video_merge.go`:
```go
package tools

import (
    "context"
    "fmt"
    "os"
    "os/exec"
    "path/filepath"
    "strings"
    "time"

    "github.com/google/uuid"
)

// VideoMergeTool merges video and audio files using FFmpeg.
// Requires ffmpeg on PATH.
type VideoMergeTool struct {
    OutputDir string
}

func (v *VideoMergeTool) Name() string { return "video_merge" }

func (v *VideoMergeTool) Description() string {
    return "Merge video and audio files using FFmpeg. Modes: 'mux_audio' (add audio track to video), 'concat' (sequential join). Requires ffmpeg on PATH."
}

func (v *VideoMergeTool) InputSchema() map[string]any {
    return map[string]any{
        "type": "object",
        "properties": map[string]any{
            "inputs": map[string]any{
                "type":        "array",
                "description": "Array of input file paths. For mux_audio: [video_path, audio_path]. For concat: all video paths in order.",
                "items":       map[string]any{"type": "string"},
            },
            "mode": map[string]any{
                "type":        "string",
                "enum":        []any{"mux_audio", "concat"},
                "description": "Merge mode: 'mux_audio' adds an audio track to video; 'concat' joins clips sequentially",
            },
            "output_format": map[string]any{
                "type":        "string",
                "description": "Output container format (default: mp4)",
            },
        },
        "required": []any{"inputs"},
    }
}

func (v *VideoMergeTool) Execute(ctx context.Context, input any) (any, error) {
    args, ok := input.(map[string]any)
    if !ok {
        return nil, fmt.Errorf("video_merge: invalid input")
    }

    rawInputs, ok := args["inputs"]
    if !ok {
        return nil, fmt.Errorf("video_merge: 'inputs' is required")
    }
    inputsArr, ok := rawInputs.([]any)
    if !ok {
        return nil, fmt.Errorf("video_merge: 'inputs' must be an array")
    }
    if len(inputsArr) == 0 {
        return nil, fmt.Errorf("video_merge: 'inputs' must have at least one element")
    }

    var inputs []string
    for _, p := range inputsArr {
        s, ok := p.(string)
        if !ok || s == "" {
            return nil, fmt.Errorf("video_merge: all inputs must be non-empty strings")
        }
        inputs = append(inputs, s)
    }

    mode, _ := args["mode"].(string)
    if mode == "" {
        mode = "mux_audio"
    }
    format, _ := args["output_format"].(string)
    if format == "" {
        format = "mp4"
    }

    outDir := v.OutputDir
    if outDir == "" {
        outDir = os.TempDir()
    }
    if err := os.MkdirAll(outDir, 0755); err != nil {
        return nil, fmt.Errorf("video_merge: create output dir: %w", err)
    }
    outPath := filepath.Join(outDir, uuid.New().String()+"."+format)

    var ffmpegArgs []string
    switch mode {
    case "mux_audio":
        if len(inputs) < 2 {
            return nil, fmt.Errorf("video_merge: mux_audio requires [video, audio]")
        }
        ffmpegArgs = []string{
            "-i", inputs[0],
            "-i", inputs[1],
            "-c:v", "copy",
            "-c:a", "aac",
            "-map", "0:v:0",
            "-map", "1:a:0",
            "-shortest",
            outPath,
        }
    case "concat":
        // Write concat list file
        listPath := filepath.Join(outDir, uuid.New().String()+".txt")
        var sb strings.Builder
        for _, p := range inputs {
            fmt.Fprintf(&sb, "file '%s'\n", p)
        }
        if err := os.WriteFile(listPath, []byte(sb.String()), 0644); err != nil {
            return nil, fmt.Errorf("video_merge: write concat list: %w", err)
        }
        defer os.Remove(listPath)
        ffmpegArgs = []string{
            "-f", "concat",
            "-safe", "0",
            "-i", listPath,
            "-c", "copy",
            outPath,
        }
    default:
        return nil, fmt.Errorf("video_merge: unknown mode %q (supported: mux_audio, concat)", mode)
    }

    execCtx, cancel := context.WithTimeout(ctx, 10*time.Minute)
    defer cancel()

    cmd := exec.CommandContext(execCtx, "ffmpeg", append([]string{"-y"}, ffmpegArgs...)...)
    out, err := cmd.CombinedOutput()
    if err != nil {
        return nil, fmt.Errorf("video_merge: ffmpeg failed: %w\n%s", err, string(out))
    }

    return outPath, nil
}
```

**Step 4: Run tests**

```bash
go test ./internal/tools/... -run TestVideoMerge -v -race
```
Expected: PASS (validation tests pass; actual FFmpeg tests require system FFmpeg)

**Step 5: Commit**
```bash
git add internal/tools/video_merge.go internal/tools/video_merge_test.go
git commit -m "feat(tools): add VideoMergeTool — FFmpeg-based video/audio merging"
```

---

### Task 9: Implement remotion_render tool

Saves composition code to a temp file and runs `@remotion/renderer` to produce an mp4.

**Files:**
- Create: `internal/tools/remotion_render.go`
- Create: `internal/tools/remotion_render_test.go`

**Step 1: Write failing test (validation only)**

Create `internal/tools/remotion_render_test.go`:
```go
package tools

import (
    "context"
    "testing"
)

func TestRemotion_Name(t *testing.T) {
    tool := &RemotionRenderTool{}
    if tool.Name() != "remotion_render" {
        t.Errorf("expected 'remotion_render', got %q", tool.Name())
    }
}

func TestRemotion_InputValidation(t *testing.T) {
    tool := &RemotionRenderTool{}

    t.Run("missing composition_code", func(t *testing.T) {
        _, err := tool.Execute(context.Background(), map[string]any{
            "audio_path": "/tmp/test.mp3",
        })
        if err == nil {
            t.Error("expected error for missing composition_code")
        }
    })

    t.Run("empty composition_code", func(t *testing.T) {
        _, err := tool.Execute(context.Background(), map[string]any{
            "composition_code": "",
        })
        if err == nil {
            t.Error("expected error for empty composition_code")
        }
    })
}

func TestRemotion_Schema(t *testing.T) {
    tool := &RemotionRenderTool{}
    schema := tool.InputSchema()
    props, ok := schema["properties"].(map[string]any)
    if !ok {
        t.Fatal("missing properties")
    }
    for _, field := range []string{"composition_code", "audio_path", "duration_sec", "fps"} {
        if _, ok := props[field]; !ok {
            t.Errorf("missing property %q", field)
        }
    }
}
```

**Step 2: Run test to confirm it fails**

```bash
go test ./internal/tools/... -run TestRemotion -v
```
Expected: FAIL — `RemotionRenderTool undefined`

**Step 3: Implement RemotionRenderTool**

Create `internal/tools/remotion_render.go`:
```go
package tools

import (
    "context"
    "fmt"
    "os"
    "os/exec"
    "path/filepath"
    "time"

    "github.com/google/uuid"
)

// RemotionRenderTool renders a Remotion React composition to video.
// Requires Node.js and @remotion/renderer installed in the working directory
// or globally. The composition code is written to a temp directory and
// rendered via `npx remotion render`.
type RemotionRenderTool struct {
    OutputDir    string
    RemotionRoot string // path to Remotion project root (default: ./remotion)
}

func (r *RemotionRenderTool) Name() string { return "remotion_render" }

func (r *RemotionRenderTool) Description() string {
    return "Render a Remotion React composition to an MP4 video file. Requires Node.js and @remotion/renderer on the host. composition_code is the full React/Remotion source. audio_path is the TTS audio file to use."
}

func (r *RemotionRenderTool) InputSchema() map[string]any {
    return map[string]any{
        "type": "object",
        "properties": map[string]any{
            "composition_code": map[string]any{
                "type":        "string",
                "description": "Full Remotion React composition source code (TypeScript/JavaScript)",
            },
            "audio_path": map[string]any{
                "type":        "string",
                "description": "Path to the TTS audio file to include in the video",
            },
            "duration_sec": map[string]any{
                "type":        "number",
                "description": "Video duration in seconds (default: 60)",
            },
            "fps": map[string]any{
                "type":        "number",
                "description": "Frames per second (default: 30)",
            },
            "width": map[string]any{
                "type":        "number",
                "description": "Video width in pixels (default: 1920)",
            },
            "height": map[string]any{
                "type":        "number",
                "description": "Video height in pixels (default: 1080)",
            },
        },
        "required": []any{"composition_code"},
    }
}

func (r *RemotionRenderTool) Execute(ctx context.Context, input any) (any, error) {
    args, ok := input.(map[string]any)
    if !ok {
        return nil, fmt.Errorf("remotion_render: invalid input")
    }

    code, _ := args["composition_code"].(string)
    if code == "" {
        return nil, fmt.Errorf("remotion_render: composition_code is required")
    }

    audioPath, _ := args["audio_path"].(string)
    durationSec, _ := args["duration_sec"].(float64)
    if durationSec <= 0 {
        durationSec = 60
    }
    fps, _ := args["fps"].(float64)
    if fps <= 0 {
        fps = 30
    }
    width, _ := args["width"].(float64)
    if width <= 0 {
        width = 1920
    }
    height, _ := args["height"].(float64)
    if height <= 0 {
        height = 1080
    }

    // Write composition code to temp dir alongside audio reference
    tmpDir, err := os.MkdirTemp("", "upal-remotion-*")
    if err != nil {
        return nil, fmt.Errorf("remotion_render: create temp dir: %w", err)
    }
    defer os.RemoveAll(tmpDir)

    codePath := filepath.Join(tmpDir, "Composition.tsx")
    if err := os.WriteFile(codePath, []byte(code), 0644); err != nil {
        return nil, fmt.Errorf("remotion_render: write composition code: %w", err)
    }

    outDir := r.OutputDir
    if outDir == "" {
        outDir = os.TempDir()
    }
    if err := os.MkdirAll(outDir, 0755); err != nil {
        return nil, fmt.Errorf("remotion_render: create output dir: %w", err)
    }
    outPath := filepath.Join(outDir, uuid.New().String()+".mp4")

    remotionRoot := r.RemotionRoot
    if remotionRoot == "" {
        remotionRoot = "./remotion"
    }

    // Build props JSON to pass to the composition
    propsJSON := fmt.Sprintf(`{"audioPath":%q,"durationInSeconds":%g}`, audioPath, durationSec)

    npxArgs := []string{
        "remotion", "render",
        remotionRoot,           // Remotion project root
        "Composition",          // composition ID (convention)
        outPath,
        fmt.Sprintf("--width=%d", int(width)),
        fmt.Sprintf("--height=%d", int(height)),
        fmt.Sprintf("--fps=%d", int(fps)),
        fmt.Sprintf("--frames=0-%d", int(durationSec*fps)-1),
        "--props=" + propsJSON,
        "--overwrite",
    }

    execCtx, cancel := context.WithTimeout(ctx, 30*time.Minute)
    defer cancel()

    cmd := exec.CommandContext(execCtx, "npx", npxArgs...)
    cmd.Env = append(os.Environ(), "COMPOSITION_FILE="+codePath)
    out, err := cmd.CombinedOutput()
    if err != nil {
        return nil, fmt.Errorf("remotion_render: render failed: %w\n%s", err, string(out))
    }

    return outPath, nil
}
```

**Step 4: Run tests**

```bash
go test ./internal/tools/... -run TestRemotion -v -race
```
Expected: PASS (validation tests pass)

**Step 5: Commit**
```bash
git add internal/tools/remotion_render.go internal/tools/remotion_render_test.go
git commit -m "feat(tools): add RemotionRenderTool — renders React compositions to MP4"
```

---

### Task 10: Register new tools and wire OutputDir into VideoMerge and RemotionRender

**Files:**
- Modify: `cmd/upal/main.go`

**Step 1: Register tools after outputDir creation**

In `main.go`, after `toolReg.Register(tools.NewPublishTool(...))`:
```go
toolReg.Register(&tools.VideoMergeTool{OutputDir: outputDir})
toolReg.Register(&tools.RemotionRenderTool{OutputDir: outputDir})
```

**Step 2: Compile and test**

```bash
go build ./... && go test ./... -race
```
Expected: PASS

**Step 3: Commit**
```bash
git add cmd/upal/main.go
git commit -m "feat(main): register video_merge and remotion_render tools"
```

---

### Task 11: Update skill files

Update LLM-facing documentation so the generator knows how to use TTS models and new tools.

**Files:**
- Modify: `internal/skills/nodes/agent-node.md`
- Create: `internal/skills/tools/tool-tts.md` (dir may need creating)
- Create: `internal/skills/tools/tool-remotion_render.md`
- Create: `internal/skills/tools/tool-video_merge.md`

**Step 1: Add TTS section to agent-node.md**

In `internal/skills/nodes/agent-node.md`, in the Tool Usage Guide table, add:

```markdown
| Convert text script to spoken audio | `tts` (via tts model) |
```

Below the table add:

```markdown
### TTS model nodes

When the task requires speech synthesis, use a TTS model instead of a text model:

| Field | Value |
|-------|-------|
| `model` | `"openai/tts-1-hd"` or `"openai/tts-1"` |
| `system_prompt` | Speaking instructions: tone, pace, emotion per section |
| `prompt` | Text to speak — always reference upstream script via `{{script_node}}` |

TTS nodes do NOT support `tools`. The output is stored as a file path string in session state.
```

**Step 2: Create tool skill files directory if needed**

```bash
mkdir -p internal/skills/tools
```

**Step 3: Create tool-tts.md** (for reference if tts is used as a tool in future)

Create `internal/skills/tools/tool-tts.md`:
```markdown
---
name: tool-tts
description: TTS is implemented as a model category, not a tool. Use a TTS agent node.
---

TTS (text-to-speech) in Upal is implemented as a model-type node, following the same pattern as image generation. Use an agent node with a TTS model:

- `model`: `"openai/tts-1-hd"`
- `system_prompt`: speaking style instructions
- `prompt`: `{{upstream_script_node}}`

Output: file path string stored in session state, accessible via `{{tts_node}}`.
```

**Step 4: Create tool-remotion_render.md**

Create `internal/skills/tools/tool-remotion_render.md`:
```markdown
---
name: tool-remotion_render
description: Guide for tool node using remotion_render — renders Remotion React compositions to MP4
---

## Objective

Execute a Remotion render from a React composition code string. Requires an upstream agent that generates the composition code, plus a TTS audio file path.

## Schema

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `composition_code` | string | Yes | Full Remotion React/TSX composition source |
| `audio_path` | string | No | TTS output file path, e.g. `{{tts_node}}` |
| `duration_sec` | number | No | Video duration in seconds (default: 60) |
| `fps` | number | No | Frames per second (default: 30) |
| `width` | number | No | Width in pixels (default: 1920) |
| `height` | number | No | Height in pixels (default: 1080) |

## Output

File path string to the rendered `.mp4`, e.g. `/tmp/upal-outputs/abc123.mp4`.

## Typical pipeline position

```
script_agent → tts_node → remotion_compose_agent → remotion_render (tool) → output
```

## Rules

1. `composition_code` must be complete, renderable Remotion source — not a snippet.
2. `audio_path` should reference the TTS node output: `{{tts_node}}`.
3. The upstream remotion_compose agent MUST be instructed to output ONLY the code, no markdown fences.
4. Host must have Node.js and `@remotion/renderer` installed.
```

**Step 5: Create tool-video_merge.md**

Create `internal/skills/tools/tool-video_merge.md`:
```markdown
---
name: tool-video_merge
description: Guide for tool node using video_merge — merges video and audio files with FFmpeg
---

## Objective

Combine media files using FFmpeg. Use `mux_audio` to add a TTS audio track to a video, or `concat` to join clips sequentially.

## Schema

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `inputs` | array of strings | Yes | File paths to merge. For `mux_audio`: `[video_path, audio_path]` |
| `mode` | string | No | `"mux_audio"` (default) or `"concat"` |
| `output_format` | string | No | Container format (default: `"mp4"`) |

## Output

File path string to the merged file, e.g. `/tmp/upal-outputs/abc123.mp4`.

## Examples

### Add TTS audio to Remotion video
```json
{
  "tool": "video_merge",
  "input": {
    "inputs": ["{{remotion_render_node}}", "{{tts_node}}"],
    "mode": "mux_audio"
  }
}
```

### Concatenate clips
```json
{
  "tool": "video_merge",
  "input": {
    "inputs": ["{{intro_node}}", "{{main_node}}", "{{outro_node}}"],
    "mode": "concat"
  }
}
```

## Rules

1. Host must have `ffmpeg` on PATH.
2. For `mux_audio`, inputs order is always `[video, audio]`.
3. Output path is returned as a plain string — reference as `{{video_merge_node}}`.
```

**Step 6: Run full test suite**

```bash
go test ./... -race
```
Expected: PASS

**Step 7: Commit**
```bash
git add internal/skills/nodes/agent-node.md internal/skills/tools/
git commit -m "docs(skills): add TTS model guidance and remotion_render/video_merge tool skill files"
```

---

## Final Verification

```bash
go build ./...
go test ./... -race -count=1
make test-frontend
```

All pass → implementation complete.
